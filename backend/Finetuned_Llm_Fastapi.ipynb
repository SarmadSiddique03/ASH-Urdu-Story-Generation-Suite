{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IVmj0-ysWAIf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b6lzKuh2jLjh",
    "outputId": "a724d81e-e2b1-4518-9d7c-1895c9a48bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.4.26)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m95.2/95.2 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m72.0/72.0 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: uvicorn, python-multipart, pyngrok, starlette, fastapi\n",
      "Successfully installed fastapi-0.115.12 pyngrok-7.2.8 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi nest_asyncio uvicorn pyngrok diffusers transformers torch accelerate python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "miht7-nIWG6p",
    "outputId": "6c3a1199-dc76-48cd-946c-e89498780095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9WkxnuoqXoz"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# ‚úÖ Auto-detect best dtype\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# ‚úÖ Use 4-bit quantization if on GPU (reduces VRAM usage)\n",
    "load_in_4bit = torch.cuda.is_available()\n",
    "\n",
    "# ‚úÖ Load the fine-tuned model and tokenizer\n",
    "model_name = \"sarmadsiddiqui29/Llama-3.1-8B-Instruct-Urdu-Story\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=\"\",\n",
    ")\n",
    "\n",
    "# ‚úÖ Ensure model is on the correct device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ‚úÖ Confirm everything is set correctly\n",
    "print(f\"Model loaded on {device} with dtype={dtype} (4-bit={load_in_4bit})\")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OcBgXc1sdDMl",
    "outputId": "913ace85-3af1-4807-c8a4-f54306c053a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\n",
      "You can set it like this: export NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Linux/macOS) or set NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Windows)\n",
      "Starting ngrok tunnel for port 8000...\n",
      " FastAPI app is exposed at: http://3576-34-87-126-232.ngrok-free.app\n",
      "Access the FastAPI docs at: http://3576-34-87-126-232.ngrok-free.app/docs\n",
      "Starting Uvicorn server in a separate thread...\n",
      "Uvicorn server thread started. The script can now continue.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from unsloth import FastLanguageModel\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import asyncio\n",
    "from threading import Thread # Import Thread\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in environments like notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "\n",
    "def fix_spacing(text):\n",
    "    \"\"\"Fix missing spaces in Urdu text.\"\"\"\n",
    "    # Using the updated regex for Urdu characters\n",
    "    return re.sub(r'(?<=[ÿÄ-€ø])(?=[ÿÄ-€ø])', ' ', text)\n",
    "\n",
    "def extract_text_after_last_story(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text after the last occurrence of \"Story:\" and ensures it ends with \"€î\"\n",
    "    \"\"\"\n",
    "    matches = [m.end() for m in re.finditer(r'(?i)Story:', text)]\n",
    "    if matches:\n",
    "        extracted_text = text[matches[-1]:].strip()\n",
    "        # Using the correct Urdu full stop\n",
    "        last_full_stop = extracted_text.rfind(\"€î\")\n",
    "        if last_full_stop != -1:\n",
    "            return extracted_text[:last_full_stop + 1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def remove_duplicate_sentences(text: str) -> str:\n",
    "    \"\"\"Removes duplicate sentences from the text based on the Urdu full stop '€î'.\"\"\"\n",
    "    # Using the correct Urdu full stop\n",
    "    sentences = text.split(\"€î\")\n",
    "    seen = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence not in seen:\n",
    "            seen.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    # Using the correct Urdu full stop for joining\n",
    "    return \"€î \".join(cleaned_sentences) + \"€î\" if cleaned_sentences else \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Story Generation Function\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def generate_story_outline(concept: str, initial_story: str = \"\", max_steps: int = 9) -> str:\n",
    "    \"\"\"\n",
    "    Generates a structured, coherent, and grammatically sound Urdu story iteratively.\n",
    "    Uses a step-wise template to build narrative depth, with explicit instructions to ensure\n",
    "    a complete and engaging journey that concludes definitively.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None or device is None:\n",
    "        return \"Model or tokenizer not loaded. Cannot generate story. Please check your environment setup and model paths/permissions.\"\n",
    "\n",
    "    story_text = initial_story\n",
    "    complete_story = initial_story\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        if step == 1:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "[üîπ ÿ¢ÿ∫ÿßÿ≤ ⁄©ÿ±€å⁄∫:\n",
    "- ÿ¨ŸÖŸÑ€í ⁄Ü⁄æŸàŸπ€í ÿßŸàÿ± ÿ®ÿ±ÿß€Å ÿ±ÿßÿ≥ÿ™ €ÅŸà⁄∫€î\n",
    "- ⁄©€ÅÿßŸÜ€å ŸÖ€å⁄∫ ÿßÿ≥ÿ±ÿßÿ± ÿßŸàÿ± ÿØŸÑ⁄Üÿ≥Ÿæ€å Ÿæ€åÿØÿß ⁄©ÿ±€å⁄∫€î\n",
    "- ÿµÿ±ŸÅ ÿßÿ±ÿØŸà ŸÖ€å⁄∫ ⁄©€ÅÿßŸÜ€å ÿ¥ÿ±Ÿàÿπ ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©ÿß ÿØŸÑ⁄©ÿ¥ ÿ¢ÿ∫ÿßÿ≤ ⁄©ÿ±€å⁄∫€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 2:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿ¢⁄Ø€í ÿ®⁄ë⁄æÿßÿ¶€å⁄∫:\n",
    "- ⁄©ÿ±ÿØÿßÿ± ⁄©€í ÿßÿ®ÿ™ÿØÿßÿ¶€å ÿßÿ±ÿßÿØŸà⁄∫ ÿßŸàÿ± ⁄Ü⁄æŸæ€í ÿ±ÿßÿ≤Ÿà⁄∫ ⁄©Ÿà ŸÜŸÖÿß€åÿß⁄∫ ⁄©ÿ±€å⁄∫€î\n",
    "- ⁄©€ÅÿßŸÜ€å ŸÖ€å⁄∫ ŸÖÿ≤€åÿØ ÿ≥ŸàÿßŸÑÿßÿ™ ÿßŸàÿ± ÿßÿ≥ÿ±ÿßÿ± Ÿæ€åÿØÿß ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© ŸÜ€åÿß ŸÖŸà⁄ë ÿØ€å⁄∫ ÿßŸàÿ± ⁄Ø€Åÿ±ÿßÿ¶€å Ÿæ€åÿØÿß ⁄©ÿ±€å⁄∫€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 3:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ÿ™ŸÜÿßÿ§ ÿßŸàÿ± ⁄©ÿ¥ŸÖ⁄©ÿ¥ ⁄©Ÿà ÿ®⁄ë⁄æÿßÿ¶€å⁄∫:\n",
    "- ⁄©ÿ±ÿØÿßÿ± ⁄©€å ÿØÿßÿÆŸÑ€å ⁄©ÿ¥ŸÖ⁄©ÿ¥ ÿßŸàÿ± ÿ∫€åÿ± ŸÖÿ™ŸàŸÇÿπ ŸÖŸà⁄ë ⁄©Ÿà ÿßÿ¨ÿß⁄Øÿ± ⁄©ÿ±€å⁄∫€î\n",
    "- ⁄©€ÅÿßŸÜ€å ŸÖ€å⁄∫ Ÿæ€å⁄Ü€åÿØ⁄Ø€å ÿßŸàÿ± ÿØŸÑ⁄Üÿ≥Ÿæ€å Ÿæ€åÿØÿß ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© ÿß€åÿ≥€í ŸÖŸÇÿßŸÖ Ÿæÿ± ŸÑ€í ÿ¨ÿßÿ¶€å⁄∫ ÿ¨€Åÿß⁄∫ ŸÇÿßÿ±€å ÿ≠€åÿ±ÿßŸÜ ÿ±€Å ÿ¨ÿßÿ¶€í€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 4:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ÿπÿ±Ÿàÿ¨ ⁄©€å ÿ∑ÿ±ŸÅ ÿ®⁄ë⁄æ€å⁄∫:\n",
    "- ⁄©€ÅÿßŸÜ€å ŸÖ€å⁄∫ ÿ≥ŸÜÿ≥ŸÜ€å ÿÆ€åÿ≤€å ÿßŸàÿ± ŸÜÿ¶€í ÿßŸÜ⁄©ÿ¥ÿßŸÅÿßÿ™ ÿ¥ÿßŸÖŸÑ ⁄©ÿ±€å⁄∫€î\n",
    "- ⁄©ÿ±ÿØÿßÿ±Ÿà⁄∫ ⁄©€å ÿ¨ÿØŸàÿ¨€ÅÿØ ÿßŸàÿ± ŸÖŸÇÿßÿ®ŸÑ€í ⁄©Ÿà Ÿàÿßÿ∂ÿ≠ ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© ÿπÿ±Ÿàÿ¨ Ÿæÿ± Ÿæ€ÅŸÜ⁄Üÿßÿ¶€å⁄∫ ÿ¨€Åÿß⁄∫ €Åÿ± ŸÑŸÖÿ≠€Å ŸÜ€åÿß ÿßŸÜ⁄©ÿ¥ÿßŸÅ €ÅŸà€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 5:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ŸÖŸà⁄ë ÿßŸàÿ± ŸÜ€åÿß ÿ±ÿÆ:\n",
    "- ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© ŸÜÿ¶€í ÿßŸàÿ± ÿ∫€åÿ± ŸÖÿ™ŸàŸÇÿπ ŸÖŸà⁄ë Ÿæÿ± ŸÑ€í ÿ¨ÿßÿ¶€å⁄∫€î\n",
    "- ŸÖÿ≤ÿßÿ≠ŸÖÿ™ ÿßŸàÿ± ⁄Ü€åŸÑŸÜÿ¨ÿ≤ ⁄©Ÿà ÿßÿ¨ÿß⁄Øÿ± ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ŸÖ€å⁄∫ ŸÜ€åÿß ÿ±ÿÆ ÿßŸàÿ± ŸÖÿ≤€åÿØ ⁄©ÿ¥ŸÖ⁄©ÿ¥ ÿ¥ÿßŸÖŸÑ ⁄©ÿ±€å⁄∫€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 6:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ÿßÿÆÿ™ÿ™ÿßŸÖ€å ŸÖÿ±ÿßÿ≠ŸÑ ⁄©€å ÿ∑ÿ±ŸÅ ÿ®⁄ë⁄æ€å⁄∫:\n",
    "- ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ŸÖÿ≤€åÿØ ÿ™ŸÅÿµ€åŸÑ ÿ≥€í ÿ®€åÿßŸÜ ⁄©ÿ±€å⁄∫ ÿßŸàÿ± ⁄Ü€åŸÑŸÜÿ¨ÿ≤ ⁄©€å ÿ¥ÿØÿ™ ÿ®⁄ë⁄æÿßÿ¶€å⁄∫€î\n",
    "- ⁄©ÿ±ÿØÿßÿ± ⁄©€å ÿ¨ÿØŸàÿ¨€ÅÿØ ⁄©Ÿà ⁄Ø€Åÿ±ÿßÿ¶€å ÿ≥€í Ÿæ€åÿ¥ ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿßÿÆÿ™ÿ™ÿßŸÖ€å ŸÖÿ±ÿßÿ≠ŸÑ ⁄©€å ÿ∑ÿ±ŸÅ ŸÑ€í ÿ¨ÿßÿ¶€å⁄∫ÿå ŸÖ⁄Øÿ± ÿß€å⁄© ÿ¢ÿÆÿ±€å ÿ≠€åÿ±ÿßŸÜ ⁄©ŸÜ ŸÖŸà⁄ë ⁄Ü⁄æŸà⁄ë€å⁄∫€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 7:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ŸÖ⁄©ŸÖŸÑ ÿßÿÆÿ™ÿ™ÿßŸÖ ⁄©€å ÿ™€åÿßÿ±€å:\n",
    "- ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© ŸÖÿ±ÿ®Ÿàÿ∑ ÿßŸàÿ± ŸÖ⁄©ŸÖŸÑ ÿßŸÜÿ¨ÿßŸÖ ⁄©€å ÿ∑ÿ±ŸÅ ŸÑ€í ÿ¨ÿßÿ¶€å⁄∫€î\n",
    "- ÿ™ŸÖÿßŸÖ ÿß€ÅŸÖ ŸÖŸà⁄ë ÿßŸàÿ± ⁄©ÿ¥ŸÖ⁄©ÿ¥ ⁄©Ÿà ÿ≠ŸÑ ⁄©ÿ±€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿ¥ÿßŸÜÿØÿßÿ± ÿßŸÜÿ¨ÿßŸÖ ÿ™⁄© Ÿæ€ÅŸÜ⁄Üÿßÿ¶€å⁄∫€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 8:\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ÿ¨ÿ≤ÿ¶€åÿßÿ™ ⁄©€å ÿ™ÿ±ÿ™€åÿ® ÿßŸàÿ± ÿßÿÆÿ™ÿ™ÿßŸÖ:\n",
    "- ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ŸÖ⁄©ŸÖŸÑ ⁄©ÿ±€å⁄∫ ÿßŸàÿ± ÿ¢ÿÆÿ±€å ÿ™ÿßÿ´ÿ±ÿßÿ™ ⁄Ü⁄æŸà⁄ë€å⁄∫€î\n",
    "- ÿ∫€åÿ± ÿ∂ÿ±Ÿàÿ±€å ÿ™ŸÅÿµ€åŸÑÿßÿ™ ⁄©Ÿà ÿ≠ÿ∞ŸÅ ⁄©ÿ±ÿ™€í €ÅŸàÿ¶€í ŸÖÿ±⁄©ÿ≤€å ⁄©€ÅÿßŸÜ€å Ÿæÿ± ÿ™Ÿàÿ¨€Å ÿØ€å⁄∫€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© Ÿàÿßÿ∂ÿ≠ ÿßŸàÿ± ŸÖ⁄©ŸÖŸÑ ÿßŸÜÿ¨ÿßŸÖ ⁄©€í ÿ≥ÿßÿ™⁄æ ÿÆÿ™ŸÖ ⁄©ÿ±€å⁄∫€î**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 9:  # Final polishing step with lower temperature for coherence\n",
    "            template = f\"\"\"\n",
    "**üí° ÿ®ŸÜ€åÿßÿØ€å ÿÆ€åÿßŸÑ:** {concept}\n",
    "\n",
    "⁄©€ÅÿßŸÜ€å:\n",
    "{story_text}\n",
    "\n",
    "[üîπ ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ÿß€å⁄© ŸÖ⁄©ŸÖŸÑÿå ŸÖÿ±ÿ®Ÿàÿ∑ ÿßŸàÿ± ÿØŸÑ⁄©ÿ¥ ÿßŸÜÿ¨ÿßŸÖ ÿ™⁄© Ÿæ€ÅŸÜ⁄Üÿßÿ¶€å⁄∫:\n",
    "- ⁄©€ÅÿßŸÜ€å ⁄©€í ÿ™ŸÖÿßŸÖ ÿß€ÅŸÖ ŸÖŸà⁄ë ÿßŸàÿ± ⁄©ÿ¥ŸÖ⁄©ÿ¥ ⁄©Ÿà ÿ≠ŸÑ ⁄©ÿ±€å⁄∫€î\n",
    "- ⁄©ÿ±ÿØÿßÿ± ⁄©€å ÿ∞€ÅŸÜ€å ÿßŸàÿ± ÿ¨ÿ∞ÿ®ÿßÿ™€å ÿ™ÿ±ŸÇ€å ⁄©Ÿà ÿßÿ¨ÿß⁄Øÿ± ⁄©ÿ±€å⁄∫€î\n",
    "- ÿ±Ÿàÿ≤ŸÖÿ±€Å ⁄©€å ÿ™ŸÅÿµ€åŸÑÿßÿ™ ÿ≥€í €ÅŸπ ⁄©ÿ± ÿß€å⁄© ÿØŸÑ⁄Üÿ≥Ÿæ ÿ≥ŸÅÿ± ÿßŸàÿ± ÿ™ÿ®ÿØ€åŸÑ€å ⁄©Ÿà ŸÖÿ±⁄©ÿ≤€å ÿ≠€åÿ´€åÿ™ ÿØ€å⁄∫€î\n",
    "- ⁄©€ÅÿßŸÜ€å ÿß€å⁄© Ÿàÿßÿ∂ÿ≠ÿå ÿßÿ´ÿ± ÿßŸÜ⁄Ø€åÿ≤ ÿßÿÆÿ™ÿ™ÿßŸÖ Ÿæÿ± ÿÆÿ™ŸÖ €ÅŸà€î]\n",
    "\n",
    "üìú **ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ⁄©€ÅÿßŸÜ€å ⁄©Ÿà ŸÖ⁄©ŸÖŸÑ ÿßŸàÿ± ŸÖÿ±ÿ®Ÿàÿ∑ ÿßÿÆÿ™ÿ™ÿßŸÖ ⁄©€í ÿ≥ÿßÿ™⁄æ ÿÆÿ™ŸÖ ⁄©ÿ±€å⁄∫€î**\n",
    "ÿµÿ±ŸÅ ÿßÿ±ÿØŸà ŸÖ€å⁄∫ ⁄©€ÅÿßŸÜ€å ŸÑ⁄©⁄æ€å⁄∫ ÿßŸàÿ± ÿßÿ∂ÿßŸÅ€å €ÅÿØÿß€åÿßÿ™ ÿ¥ÿßŸÖŸÑ ŸÜ€Å ⁄©ÿ±€å⁄∫€î\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "\n",
    "        # Prepare input tokens from the template\n",
    "        inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Adjust max tokens and temperature per step\n",
    "        max_tokens = 250\n",
    "        temperature = 0.7\n",
    "        if step >= 7:\n",
    "            max_tokens = 400\n",
    "        if step == 9:\n",
    "            max_tokens = 500\n",
    "            temperature = 0.6\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        # Extract the new text generated after the last occurrence of \"Story:\"\n",
    "        new_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        extracted_text = extract_text_after_last_story(new_text)\n",
    "\n",
    "        # Update story text for the next step\n",
    "        story_text = extracted_text\n",
    "        complete_story += \" \" + extracted_text\n",
    "\n",
    "    complete_story = remove_duplicate_sentences(complete_story)\n",
    "    return complete_story\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class StoryRequest(BaseModel):\n",
    "    concept: str\n",
    "    initial_story: str = \"\"\n",
    "    max_steps: int = 9\n",
    "\n",
    "@app.post(\"/generate_story/\")\n",
    "async def generate_story(request: StoryRequest):\n",
    "    try:\n",
    "        story = generate_story_outline(request.concept, request.initial_story, request.max_steps)\n",
    "        return {\"story\": story}\n",
    "    except Exception as e:\n",
    "        # Log the exception for debugging\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred during story generation: {e}\")\n",
    "\n",
    "# Code to run the FastAPI app with uvicorn and expose with ngrok\n",
    "# This pattern is suitable for environments like Google Colab or Jupyter notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the ngrok authtoken from environment variable\n",
    "    # Make sure to set the NGROK_AUTH_TOKEN environment variable\n",
    "    ngrok_auth_token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
    "    if ngrok_auth_token:\n",
    "        ngrok.set_auth_token(ngrok_auth_token)\n",
    "    else:\n",
    "        print(\"Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\")\n",
    "        print(\"You can set it like this: export NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Linux/macOS) or set NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Windows)\")\n",
    "\n",
    "\n",
    "    # Define the port for FastAPI\n",
    "    port = 8000\n",
    "\n",
    "    # Start ngrok tunnel\n",
    "    try:\n",
    "        print(f\"Starting ngrok tunnel for port {port}...\")\n",
    "        # Use bind_tls=True if you need HTTPS, but stick to False for simplicity if not required\n",
    "        public_url = ngrok.connect(port, \"http\", bind_tls=False).public_url\n",
    "        print(f\" FastAPI app is exposed at: {public_url}\")\n",
    "        print(f\"Access the FastAPI docs at: {public_url}/docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting ngrok tunnel: {e}\")\n",
    "        print(\"Please ensure ngrok is installed, authenticated, and not already running on this port.\")\n",
    "        public_url = None # Set to None if ngrok fails\n",
    "\n",
    "    # Run the FastAPI application using uvicorn in a separate thread\n",
    "    # Only start the thread if ngrok started successfully or if you intend to access locally\n",
    "    if public_url or not ngrok_auth_token: # Added condition to allow local run without ngrok token\n",
    "        try:\n",
    "            print(\"Starting Uvicorn server in a separate thread...\")\n",
    "            # Use reload=False when running with ngrok to avoid issues\n",
    "            # daemon=True allows the thread to exit when the main program exits\n",
    "            uvicorn_thread = Thread(target=uvicorn.run,\n",
    "                                    kwargs={\"app\": app, \"host\": \"0.0.0.0\", \"port\": port, \"reload\": False},\n",
    "                                    daemon=True)\n",
    "            uvicorn_thread.start()\n",
    "            print(\"Uvicorn server thread started. The script can now continue.\")\n",
    "            # If running as a script, you might want to add a line here to keep the main thread alive,\n",
    "            # e.g., input(\"Press Enter to stop the server...\\n\") or a loop\n",
    "            # In Colab, the notebook environment keeps the script alive.\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting uvicorn thread: {e}\")\n",
    "            print(\"Please check if the port is already in use.\")\n",
    "    else:\n",
    "        print(\"ngrok failed to start. Uvicorn server will not be started automatically.\")\n",
    "\n",
    "    # If running in a script that would otherwise exit, you might need a way to keep it alive\n",
    "    # This is often not needed in environments like Colab where the notebook keeps the kernel alive\n",
    "    # For a standalone script, you could add:\n",
    "    # try:\n",
    "    #     while True:\n",
    "    #         time.sleep(1)\n",
    "    # except KeyboardInterrupt:\n",
    "    #     print(\"Server stopped.\")\n",
    "    #     # Clean up ngrok tunnel on exit\n",
    "    #     if public_url:\n",
    "    #         ngrok.disconnect(public_url)\n",
    "\n",
    "\n",
    "# Instructions to run:\n",
    "# 1. Save the code above as main.py\n",
    "# 2. Set your Hugging Face token and ngrok auth token as environment variables:\n",
    "#    export HF_TOKEN=\"YOUR_HF_TOKEN\"\n",
    "#    export NGROK_AUTH_TOKEN=\"YOUR_NGROK_AUTH_TOKEN\" # Get your token from ngrok dashboard\n",
    "#    (On Windows, use set HF_TOKEN=\"YOUR_HF_TOKEN\" and set NGROK_AUTH_TOKEN=\"YOUR_NGROK_AUTH_TOKEN\")\n",
    "#    Replace 'YOUR_HF_TOKEN' and 'YOUR_NGROK_AUTH_TOKEN' with your actual tokens.\n",
    "# 3. Install the required libraries:\n",
    "#    pip install fastapi uvicorn python-multipart unsloth[cu{your_cuda_version}] transformers torch accelerate nest_asyncio pyngrok threading\n",
    "#    Note: `threading` is a built-in Python module, no need to install with pip.\n",
    "#    Replace '{your_cuda_version}' with your CUDA version (e.g., cu118, cu121). The previous CUDA errors\n",
    "#    strongly suggest an issue with your CUDA setup or the unsloth installation not matching your CUDA version.\n",
    "#    Ensure your system has the necessary CUDA libraries installed and accessible in your environment's PATH.\n",
    "# 4. Make sure you have ngrok installed (https://ngrok.com/download) and authenticated (ngrok authtoken <your_auth_token>).\n",
    "# 5. Run the python script from your terminal in the directory where you saved main.py:\n",
    "#    python main.py\n",
    "# 6. The script will print the ngrok public URL if successful, and then the \"Uvicorn server thread started...\" message.\n",
    "#    The script will not \"hang\" or block the console after starting the server thread.\n",
    "#    Use the ngrok public URL to access your FastAPI application.\n",
    "#    You can test the /generate_story/ endpoint with a POST request containing JSON data.\n",
    "#    If ngrok fails, the uvicorn server will still attempt to start locally, and you might be able to access it at http://127.0.0.1:8000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TYQxL8lHjGze"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"http://3cfe-34-125-125-99.ngrok-free.app/generate_story/\"\n",
    "\n",
    "# payload = {\n",
    "#     \"concept\": \"ÿß€åÿ≥€å ⁄©€ÅÿßŸÜ€å ŸÑ⁄©⁄æ€å⁄∫ ÿ¨ÿ≥ ŸÖ€å⁄∫ ÿß€å⁄© ÿ¥ÿÆÿµ ÿßŸÜÿ™€Åÿßÿ¶€å ÿ∫ÿ±€åÿ® €ÅŸàÿ™ÿß €Å€í€î Ÿà€Å ÿßŸæŸÜ€å ÿ≤ŸÜÿØ⁄Ø€å ÿßŸÖ€åÿ± ÿ®ŸÜŸÜ€í ⁄©€í ŸÑ€å€í ŸàŸÇŸÅ ⁄©ÿ± ÿØ€åÿ™ÿß €Å€íÿå ŸÑ€å⁄©ŸÜ ÿ¨ÿ® Ÿà€Å ÿØŸàŸÑÿ™ ŸÖŸÜÿØ ÿ®ŸÜ ÿ¨ÿßÿ™ÿß €Å€í ÿ™Ÿà ÿßÿ≥€í ÿßÿ≠ÿ≥ÿßÿ≥ €ÅŸàÿ™ÿß €Å€í ⁄©€Å ÿßÿ≥ ŸÜ€í ÿßŸæŸÜ€å ÿ≤ŸÜÿØ⁄Ø€å ŸÖ€å⁄∫ ⁄©€åÿß ⁄©⁄æŸà ÿØ€åÿß €Å€í€î\",\n",
    "#     \"initial_story\": \"\",\n",
    "#     \"max_steps\": 9\n",
    "# }\n",
    "\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "#     response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "#     print(\"Response Status Code:\", response.status_code)\n",
    "#     print(\"Response Body:\", response.json())\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"An error occurred during the request: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KTFfJQfnMci"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
