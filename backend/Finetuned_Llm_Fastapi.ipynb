{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IVmj0-ysWAIf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b6lzKuh2jLjh",
    "outputId": "a724d81e-e2b1-4518-9d7c-1895c9a48bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.4.26)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m95.2/95.2 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.0/72.0 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: uvicorn, python-multipart, pyngrok, starlette, fastapi\n",
      "Successfully installed fastapi-0.115.12 pyngrok-7.2.8 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi nest_asyncio uvicorn pyngrok diffusers transformers torch accelerate python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "miht7-nIWG6p",
    "outputId": "6c3a1199-dc76-48cd-946c-e89498780095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9WkxnuoqXoz"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ✅ Auto-detect best dtype\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# ✅ Use 4-bit quantization if on GPU (reduces VRAM usage)\n",
    "load_in_4bit = torch.cuda.is_available()\n",
    "\n",
    "# ✅ Load the fine-tuned model and tokenizer\n",
    "model_name = \"sarmadsiddiqui29/Llama-3.1-8B-Instruct-Urdu-Story\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    ")\n",
    "\n",
    "# ✅ Ensure model is on the correct device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ✅ Confirm everything is set correctly\n",
    "print(f\"Model loaded on {device} with dtype={dtype} (4-bit={load_in_4bit})\")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OcBgXc1sdDMl",
    "outputId": "913ace85-3af1-4807-c8a4-f54306c053a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\n",
      "You can set it like this: export NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Linux/macOS) or set NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Windows)\n",
      "Starting ngrok tunnel for port 8000...\n",
      " FastAPI app is exposed at: http://3576-34-87-126-232.ngrok-free.app\n",
      "Access the FastAPI docs at: http://3576-34-87-126-232.ngrok-free.app/docs\n",
      "Starting Uvicorn server in a separate thread...\n",
      "Uvicorn server thread started. The script can now continue.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from unsloth import FastLanguageModel\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import asyncio\n",
    "from threading import Thread # Import Thread\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in environments like notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "\n",
    "def fix_spacing(text):\n",
    "    \"\"\"Fix missing spaces in Urdu text.\"\"\"\n",
    "    # Using the updated regex for Urdu characters\n",
    "    return re.sub(r'(?<=[؀-ۿ])(?=[؀-ۿ])', ' ', text)\n",
    "\n",
    "def extract_text_after_last_story(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text after the last occurrence of \"Story:\" and ensures it ends with \"۔\"\n",
    "    \"\"\"\n",
    "    matches = [m.end() for m in re.finditer(r'(?i)Story:', text)]\n",
    "    if matches:\n",
    "        extracted_text = text[matches[-1]:].strip()\n",
    "        # Using the correct Urdu full stop\n",
    "        last_full_stop = extracted_text.rfind(\"۔\")\n",
    "        if last_full_stop != -1:\n",
    "            return extracted_text[:last_full_stop + 1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def remove_duplicate_sentences(text: str) -> str:\n",
    "    \"\"\"Removes duplicate sentences from the text based on the Urdu full stop '۔'.\"\"\"\n",
    "    # Using the correct Urdu full stop\n",
    "    sentences = text.split(\"۔\")\n",
    "    seen = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence not in seen:\n",
    "            seen.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    # Using the correct Urdu full stop for joining\n",
    "    return \"۔ \".join(cleaned_sentences) + \"۔\" if cleaned_sentences else \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Story Generation Function\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def generate_story_outline(concept: str, initial_story: str = \"\", max_steps: int = 9) -> str:\n",
    "    \"\"\"\n",
    "    Generates a structured, coherent, and grammatically sound Urdu story iteratively.\n",
    "    Uses a step-wise template to build narrative depth, with explicit instructions to ensure\n",
    "    a complete and engaging journey that concludes definitively.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None or device is None:\n",
    "        return \"Model or tokenizer not loaded. Cannot generate story. Please check your environment setup and model paths/permissions.\"\n",
    "\n",
    "    story_text = initial_story\n",
    "    complete_story = initial_story\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        if step == 1:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "[🔹 آغاز کریں:\n",
    "- جملے چھوٹے اور براہ راست ہوں۔\n",
    "- کہانی میں اسرار اور دلچسپی پیدا کریں۔\n",
    "- صرف اردو میں کہانی شروع کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کا دلکش آغاز کریں۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 2:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 کہانی کو آگے بڑھائیں:\n",
    "- کردار کے ابتدائی ارادوں اور چھپے رازوں کو نمایاں کریں۔\n",
    "- کہانی میں مزید سوالات اور اسرار پیدا کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو ایک نیا موڑ دیں اور گہرائی پیدا کریں۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 3:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 تناؤ اور کشمکش کو بڑھائیں:\n",
    "- کردار کی داخلی کشمکش اور غیر متوقع موڑ کو اجاگر کریں۔\n",
    "- کہانی میں پیچیدگی اور دلچسپی پیدا کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو ایک ایسے مقام پر لے جائیں جہاں قاری حیران رہ جائے۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 4:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 عروج کی طرف بڑھیں:\n",
    "- کہانی میں سنسنی خیزی اور نئے انکشافات شامل کریں۔\n",
    "- کرداروں کی جدوجہد اور مقابلے کو واضح کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو ایک عروج پر پہنچائیں جہاں ہر لمحہ نیا انکشاف ہو۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 5:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 موڑ اور نیا رخ:\n",
    "- کہانی کو ایک نئے اور غیر متوقع موڑ پر لے جائیں۔\n",
    "- مزاحمت اور چیلنجز کو اجاگر کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی میں نیا رخ اور مزید کشمکش شامل کریں۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 6:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 اختتامی مراحل کی طرف بڑھیں:\n",
    "- کہانی کو مزید تفصیل سے بیان کریں اور چیلنجز کی شدت بڑھائیں۔\n",
    "- کردار کی جدوجہد کو گہرائی سے پیش کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو اختتامی مراحل کی طرف لے جائیں، مگر ایک آخری حیران کن موڑ چھوڑیں۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 7:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 مکمل اختتام کی تیاری:\n",
    "- کہانی کو ایک مربوط اور مکمل انجام کی طرف لے جائیں۔\n",
    "- تمام اہم موڑ اور کشمکش کو حل کریں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو شاندار انجام تک پہنچائیں۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 8:\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 جزئیات کی ترتیب اور اختتام:\n",
    "- کہانی کو مکمل کریں اور آخری تاثرات چھوڑیں۔\n",
    "- غیر ضروری تفصیلات کو حذف کرتے ہوئے مرکزی کہانی پر توجہ دیں۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو ایک واضح اور مکمل انجام کے ساتھ ختم کریں۔**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 9:  # Final polishing step with lower temperature for coherence\n",
    "            template = f\"\"\"\n",
    "**💡 بنیادی خیال:** {concept}\n",
    "\n",
    "کہانی:\n",
    "{story_text}\n",
    "\n",
    "[🔹 براہ کرم کہانی کو ایک مکمل، مربوط اور دلکش انجام تک پہنچائیں:\n",
    "- کہانی کے تمام اہم موڑ اور کشمکش کو حل کریں۔\n",
    "- کردار کی ذہنی اور جذباتی ترقی کو اجاگر کریں۔\n",
    "- روزمرہ کی تفصیلات سے ہٹ کر ایک دلچسپ سفر اور تبدیلی کو مرکزی حیثیت دیں۔\n",
    "- کہانی ایک واضح، اثر انگیز اختتام پر ختم ہو۔]\n",
    "\n",
    "📜 **براہ کرم کہانی کو مکمل اور مربوط اختتام کے ساتھ ختم کریں۔**\n",
    "صرف اردو میں کہانی لکھیں اور اضافی ہدایات شامل نہ کریں۔\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "\n",
    "        # Prepare input tokens from the template\n",
    "        inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Adjust max tokens and temperature per step\n",
    "        max_tokens = 250\n",
    "        temperature = 0.7\n",
    "        if step >= 7:\n",
    "            max_tokens = 400\n",
    "        if step == 9:\n",
    "            max_tokens = 500\n",
    "            temperature = 0.6\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        # Extract the new text generated after the last occurrence of \"Story:\"\n",
    "        new_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        extracted_text = extract_text_after_last_story(new_text)\n",
    "\n",
    "        # Update story text for the next step\n",
    "        story_text = extracted_text\n",
    "        complete_story += \" \" + extracted_text\n",
    "\n",
    "    complete_story = remove_duplicate_sentences(complete_story)\n",
    "    return complete_story\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class StoryRequest(BaseModel):\n",
    "    concept: str\n",
    "    initial_story: str = \"\"\n",
    "    max_steps: int = 9\n",
    "\n",
    "@app.post(\"/generate_story/\")\n",
    "async def generate_story(request: StoryRequest):\n",
    "    try:\n",
    "        story = generate_story_outline(request.concept, request.initial_story, request.max_steps)\n",
    "        return {\"story\": story}\n",
    "    except Exception as e:\n",
    "        # Log the exception for debugging\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred during story generation: {e}\")\n",
    "\n",
    "# Code to run the FastAPI app with uvicorn and expose with ngrok\n",
    "# This pattern is suitable for environments like Google Colab or Jupyter notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the ngrok authtoken from environment variable\n",
    "    # Make sure to set the NGROK_AUTH_TOKEN environment variable\n",
    "    ngrok_auth_token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
    "    if ngrok_auth_token:\n",
    "        ngrok.set_auth_token(ngrok_auth_token)\n",
    "    else:\n",
    "        print(\"Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\")\n",
    "        print(\"You can set it like this: export NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Linux/macOS) or set NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Windows)\")\n",
    "\n",
    "\n",
    "    # Define the port for FastAPI\n",
    "    port = 8000\n",
    "\n",
    "    # Start ngrok tunnel\n",
    "    try:\n",
    "        print(f\"Starting ngrok tunnel for port {port}...\")\n",
    "        # Use bind_tls=True if you need HTTPS, but stick to False for simplicity if not required\n",
    "        public_url = ngrok.connect(port, \"http\", bind_tls=False).public_url\n",
    "        print(f\" FastAPI app is exposed at: {public_url}\")\n",
    "        print(f\"Access the FastAPI docs at: {public_url}/docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting ngrok tunnel: {e}\")\n",
    "        print(\"Please ensure ngrok is installed, authenticated, and not already running on this port.\")\n",
    "        public_url = None # Set to None if ngrok fails\n",
    "\n",
    "    # Run the FastAPI application using uvicorn in a separate thread\n",
    "    # Only start the thread if ngrok started successfully or if you intend to access locally\n",
    "    if public_url or not ngrok_auth_token: # Added condition to allow local run without ngrok token\n",
    "        try:\n",
    "            print(\"Starting Uvicorn server in a separate thread...\")\n",
    "            # Use reload=False when running with ngrok to avoid issues\n",
    "            # daemon=True allows the thread to exit when the main program exits\n",
    "            uvicorn_thread = Thread(target=uvicorn.run,\n",
    "                                    kwargs={\"app\": app, \"host\": \"0.0.0.0\", \"port\": port, \"reload\": False},\n",
    "                                    daemon=True)\n",
    "            uvicorn_thread.start()\n",
    "            print(\"Uvicorn server thread started. The script can now continue.\")\n",
    "            # If running as a script, you might want to add a line here to keep the main thread alive,\n",
    "            # e.g., input(\"Press Enter to stop the server...\\n\") or a loop\n",
    "            # In Colab, the notebook environment keeps the script alive.\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting uvicorn thread: {e}\")\n",
    "            print(\"Please check if the port is already in use.\")\n",
    "    else:\n",
    "        print(\"ngrok failed to start. Uvicorn server will not be started automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TYQxL8lHjGze"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"http://3cfe-34-125-125-99.ngrok-free.app/generate_story/\"\n",
    "\n",
    "# payload = {\n",
    "#     \"concept\": \"ایسی کہانی لکھیں جس میں ایک شخص انتہائی غریب ہوتا ہے۔ وہ اپنی زندگی امیر بننے کے لیے وقف کر دیتا ہے، لیکن جب وہ دولت مند بن جاتا ہے تو اسے احساس ہوتا ہے کہ اس نے اپنی زندگی میں کیا کھو دیا ہے۔\",\n",
    "#     \"initial_story\": \"\",\n",
    "#     \"max_steps\": 9\n",
    "# }\n",
    "\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "#     response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "#     print(\"Response Status Code:\", response.status_code)\n",
    "#     print(\"Response Body:\", response.json())\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"An error occurred during the request: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KTFfJQfnMci"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
