{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IVmj0-ysWAIf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b6lzKuh2jLjh",
    "outputId": "a724d81e-e2b1-4518-9d7c-1895c9a48bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.4.26)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m95.2/95.2 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m72.0/72.0 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: uvicorn, python-multipart, pyngrok, starlette, fastapi\n",
      "Successfully installed fastapi-0.115.12 pyngrok-7.2.8 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi nest_asyncio uvicorn pyngrok diffusers transformers torch accelerate python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "miht7-nIWG6p",
    "outputId": "6c3a1199-dc76-48cd-946c-e89498780095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9WkxnuoqXoz"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# âœ… Auto-detect best dtype\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# âœ… Use 4-bit quantization if on GPU (reduces VRAM usage)\n",
    "load_in_4bit = torch.cuda.is_available()\n",
    "\n",
    "# âœ… Load the fine-tuned model and tokenizer\n",
    "model_name = \"sarmadsiddiqui29/Llama-3.1-8B-Instruct-Urdu-Story\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    ")\n",
    "\n",
    "# âœ… Ensure model is on the correct device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# âœ… Confirm everything is set correctly\n",
    "print(f\"Model loaded on {device} with dtype={dtype} (4-bit={load_in_4bit})\")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OcBgXc1sdDMl",
    "outputId": "913ace85-3af1-4807-c8a4-f54306c053a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\n",
      "You can set it like this: export NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Linux/macOS) or set NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Windows)\n",
      "Starting ngrok tunnel for port 8000...\n",
      " FastAPI app is exposed at: http://3576-34-87-126-232.ngrok-free.app\n",
      "Access the FastAPI docs at: http://3576-34-87-126-232.ngrok-free.app/docs\n",
      "Starting Uvicorn server in a separate thread...\n",
      "Uvicorn server thread started. The script can now continue.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from unsloth import FastLanguageModel\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import asyncio\n",
    "from threading import Thread # Import Thread\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in environments like notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "\n",
    "def fix_spacing(text):\n",
    "    \"\"\"Fix missing spaces in Urdu text.\"\"\"\n",
    "    # Using the updated regex for Urdu characters\n",
    "    return re.sub(r'(?<=[Ø€-Û¿])(?=[Ø€-Û¿])', ' ', text)\n",
    "\n",
    "def extract_text_after_last_story(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text after the last occurrence of \"Story:\" and ensures it ends with \"Û”\"\n",
    "    \"\"\"\n",
    "    matches = [m.end() for m in re.finditer(r'(?i)Story:', text)]\n",
    "    if matches:\n",
    "        extracted_text = text[matches[-1]:].strip()\n",
    "        # Using the correct Urdu full stop\n",
    "        last_full_stop = extracted_text.rfind(\"Û”\")\n",
    "        if last_full_stop != -1:\n",
    "            return extracted_text[:last_full_stop + 1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def remove_duplicate_sentences(text: str) -> str:\n",
    "    \"\"\"Removes duplicate sentences from the text based on the Urdu full stop 'Û”'.\"\"\"\n",
    "    # Using the correct Urdu full stop\n",
    "    sentences = text.split(\"Û”\")\n",
    "    seen = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence not in seen:\n",
    "            seen.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    # Using the correct Urdu full stop for joining\n",
    "    return \"Û” \".join(cleaned_sentences) + \"Û”\" if cleaned_sentences else \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Story Generation Function\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def generate_story_outline(concept: str, initial_story: str = \"\", max_steps: int = 9) -> str:\n",
    "    \"\"\"\n",
    "    Generates a structured, coherent, and grammatically sound Urdu story iteratively.\n",
    "    Uses a step-wise template to build narrative depth, with explicit instructions to ensure\n",
    "    a complete and engaging journey that concludes definitively.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None or device is None:\n",
    "        return \"Model or tokenizer not loaded. Cannot generate story. Please check your environment setup and model paths/permissions.\"\n",
    "\n",
    "    story_text = initial_story\n",
    "    complete_story = initial_story\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        if step == 1:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "[ğŸ”¹ Ø¢ØºØ§Ø² Ú©Ø±ÛŒÚº:\n",
    "- Ø¬Ù…Ù„Û’ Ú†Ú¾ÙˆÙ¹Û’ Ø§ÙˆØ± Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª ÛÙˆÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ø§Ø³Ø±Ø§Ø± Ø§ÙˆØ± Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”\n",
    "- ØµØ±Ù Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ú©ÛØ§Ù†ÛŒ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ø§ Ø¯Ù„Ú©Ø´ Ø¢ØºØ§Ø² Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 2:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø¢Ú¯Û’ Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚº:\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©Û’ Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ø§Ø±Ø§Ø¯ÙˆÚº Ø§ÙˆØ± Ú†Ú¾Ù¾Û’ Ø±Ø§Ø²ÙˆÚº Ú©Ùˆ Ù†Ù…Ø§ÛŒØ§Úº Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ù…Ø²ÛŒØ¯ Ø³ÙˆØ§Ù„Ø§Øª Ø§ÙˆØ± Ø§Ø³Ø±Ø§Ø± Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù†ÛŒØ§ Ù…ÙˆÚ‘ Ø¯ÛŒÚº Ø§ÙˆØ± Ú¯ÛØ±Ø§Ø¦ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 3:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ ØªÙ†Ø§Ø¤ Ø§ÙˆØ± Ú©Ø´Ù…Ú©Ø´ Ú©Ùˆ Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚº:\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ú©Ø´Ù…Ú©Ø´ Ø§ÙˆØ± ØºÛŒØ± Ù…ØªÙˆÙ‚Ø¹ Ù…ÙˆÚ‘ Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø§ÙˆØ± Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ø§ÛŒØ³Û’ Ù…Ù‚Ø§Ù… Ù¾Ø± Ù„Û’ Ø¬Ø§Ø¦ÛŒÚº Ø¬ÛØ§Úº Ù‚Ø§Ø±ÛŒ Ø­ÛŒØ±Ø§Ù† Ø±Û Ø¬Ø§Ø¦Û’Û”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 4:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ø¹Ø±ÙˆØ¬ Ú©ÛŒ Ø·Ø±Ù Ø¨Ú‘Ú¾ÛŒÚº:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ø³Ù†Ø³Ù†ÛŒ Ø®ÛŒØ²ÛŒ Ø§ÙˆØ± Ù†Ø¦Û’ Ø§Ù†Ú©Ø´Ø§ÙØ§Øª Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©Ø±Ø¯Ø§Ø±ÙˆÚº Ú©ÛŒ Ø¬Ø¯ÙˆØ¬ÛØ¯ Ø§ÙˆØ± Ù…Ù‚Ø§Ø¨Ù„Û’ Ú©Ùˆ ÙˆØ§Ø¶Ø­ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ø¹Ø±ÙˆØ¬ Ù¾Ø± Ù¾ÛÙ†Ú†Ø§Ø¦ÛŒÚº Ø¬ÛØ§Úº ÛØ± Ù„Ù…Ø­Û Ù†ÛŒØ§ Ø§Ù†Ú©Ø´Ø§Ù ÛÙˆÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 5:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ù…ÙˆÚ‘ Ø§ÙˆØ± Ù†ÛŒØ§ Ø±Ø®:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù†Ø¦Û’ Ø§ÙˆØ± ØºÛŒØ± Ù…ØªÙˆÙ‚Ø¹ Ù…ÙˆÚ‘ Ù¾Ø± Ù„Û’ Ø¬Ø§Ø¦ÛŒÚºÛ”\n",
    "- Ù…Ø²Ø§Ø­Ù…Øª Ø§ÙˆØ± Ú†ÛŒÙ„Ù†Ø¬Ø² Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ù†ÛŒØ§ Ø±Ø® Ø§ÙˆØ± Ù…Ø²ÛŒØ¯ Ú©Ø´Ù…Ú©Ø´ Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 6:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ø§Ø®ØªØªØ§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ú©ÛŒ Ø·Ø±Ù Ø¨Ú‘Ú¾ÛŒÚº:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ù…Ø²ÛŒØ¯ ØªÙØµÛŒÙ„ Ø³Û’ Ø¨ÛŒØ§Ù† Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ú†ÛŒÙ„Ù†Ø¬Ø² Ú©ÛŒ Ø´Ø¯Øª Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚºÛ”\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒ Ø¬Ø¯ÙˆØ¬ÛØ¯ Ú©Ùˆ Ú¯ÛØ±Ø§Ø¦ÛŒ Ø³Û’ Ù¾ÛŒØ´ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§Ø®ØªØªØ§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ú©ÛŒ Ø·Ø±Ù Ù„Û’ Ø¬Ø§Ø¦ÛŒÚºØŒ Ù…Ú¯Ø± Ø§ÛŒÚ© Ø¢Ø®Ø±ÛŒ Ø­ÛŒØ±Ø§Ù† Ú©Ù† Ù…ÙˆÚ‘ Ú†Ú¾ÙˆÚ‘ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 7:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ù…Ú©Ù…Ù„ Ø§Ø®ØªØªØ§Ù… Ú©ÛŒ ØªÛŒØ§Ø±ÛŒ:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù…Ø±Ø¨ÙˆØ· Ø§ÙˆØ± Ù…Ú©Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ú©ÛŒ Ø·Ø±Ù Ù„Û’ Ø¬Ø§Ø¦ÛŒÚºÛ”\n",
    "- ØªÙ…Ø§Ù… Ø§ÛÙ… Ù…ÙˆÚ‘ Ø§ÙˆØ± Ú©Ø´Ù…Ú©Ø´ Ú©Ùˆ Ø­Ù„ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø´Ø§Ù†Ø¯Ø§Ø± Ø§Ù†Ø¬Ø§Ù… ØªÚ© Ù¾ÛÙ†Ú†Ø§Ø¦ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 8:\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù…:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¢Ø®Ø±ÛŒ ØªØ§Ø«Ø±Ø§Øª Ú†Ú¾ÙˆÚ‘ÛŒÚºÛ”\n",
    "- ØºÛŒØ± Ø¶Ø±ÙˆØ±ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ú©Ùˆ Ø­Ø°Ù Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…Ø±Ú©Ø²ÛŒ Ú©ÛØ§Ù†ÛŒ Ù¾Ø± ØªÙˆØ¬Û Ø¯ÛŒÚºÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© ÙˆØ§Ø¶Ø­ Ø§ÙˆØ± Ù…Ú©Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ú©Û’ Ø³Ø§ØªÚ¾ Ø®ØªÙ… Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 9:  # Final polishing step with lower temperature for coherence\n",
    "            template = f\"\"\"\n",
    "**ğŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ğŸ”¹ Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ØŒ Ù…Ø±Ø¨ÙˆØ· Ø§ÙˆØ± Ø¯Ù„Ú©Ø´ Ø§Ù†Ø¬Ø§Ù… ØªÚ© Ù¾ÛÙ†Ú†Ø§Ø¦ÛŒÚº:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Û’ ØªÙ…Ø§Ù… Ø§ÛÙ… Ù…ÙˆÚ‘ Ø§ÙˆØ± Ú©Ø´Ù…Ú©Ø´ Ú©Ùˆ Ø­Ù„ Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒ Ø°ÛÙ†ÛŒ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ±Ù‚ÛŒ Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ÛŒÚºÛ”\n",
    "- Ø±ÙˆØ²Ù…Ø±Û Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ø³Û’ ÛÙ¹ Ú©Ø± Ø§ÛŒÚ© Ø¯Ù„Ú†Ø³Ù¾ Ø³ÙØ± Ø§ÙˆØ± ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Ùˆ Ù…Ø±Ú©Ø²ÛŒ Ø­ÛŒØ«ÛŒØª Ø¯ÛŒÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ø§ÛŒÚ© ÙˆØ§Ø¶Ø­ØŒ Ø§Ø«Ø± Ø§Ù†Ú¯ÛŒØ² Ø§Ø®ØªØªØ§Ù… Ù¾Ø± Ø®ØªÙ… ÛÙˆÛ”]\n",
    "\n",
    "ğŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø§ÙˆØ± Ù…Ø±Ø¨ÙˆØ· Ø§Ø®ØªØªØ§Ù… Ú©Û’ Ø³Ø§ØªÚ¾ Ø®ØªÙ… Ú©Ø±ÛŒÚºÛ”**\n",
    "ØµØ±Ù Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ú©ÛØ§Ù†ÛŒ Ù„Ú©Ú¾ÛŒÚº Ø§ÙˆØ± Ø§Ø¶Ø§ÙÛŒ ÛØ¯Ø§ÛŒØ§Øª Ø´Ø§Ù…Ù„ Ù†Û Ú©Ø±ÛŒÚºÛ”\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "\n",
    "        # Prepare input tokens from the template\n",
    "        inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Adjust max tokens and temperature per step\n",
    "        max_tokens = 250\n",
    "        temperature = 0.7\n",
    "        if step >= 7:\n",
    "            max_tokens = 400\n",
    "        if step == 9:\n",
    "            max_tokens = 500\n",
    "            temperature = 0.6\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        # Extract the new text generated after the last occurrence of \"Story:\"\n",
    "        new_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        extracted_text = extract_text_after_last_story(new_text)\n",
    "\n",
    "        # Update story text for the next step\n",
    "        story_text = extracted_text\n",
    "        complete_story += \" \" + extracted_text\n",
    "\n",
    "    complete_story = remove_duplicate_sentences(complete_story)\n",
    "    return complete_story\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class StoryRequest(BaseModel):\n",
    "    concept: str\n",
    "    initial_story: str = \"\"\n",
    "    max_steps: int = 9\n",
    "\n",
    "@app.post(\"/generate_story/\")\n",
    "async def generate_story(request: StoryRequest):\n",
    "    try:\n",
    "        story = generate_story_outline(request.concept, request.initial_story, request.max_steps)\n",
    "        return {\"story\": story}\n",
    "    except Exception as e:\n",
    "        # Log the exception for debugging\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred during story generation: {e}\")\n",
    "\n",
    "# Code to run the FastAPI app with uvicorn and expose with ngrok\n",
    "# This pattern is suitable for environments like Google Colab or Jupyter notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the ngrok authtoken from environment variable\n",
    "    # Make sure to set the NGROK_AUTH_TOKEN environment variable\n",
    "    ngrok_auth_token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
    "    if ngrok_auth_token:\n",
    "        ngrok.set_auth_token(ngrok_auth_token)\n",
    "    else:\n",
    "        print(\"Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\")\n",
    "        print(\"You can set it like this: export NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Linux/macOS) or set NGROK_AUTH_TOKEN='YOUR_NGROK_AUTH_TOKEN' (Windows)\")\n",
    "\n",
    "\n",
    "    # Define the port for FastAPI\n",
    "    port = 8000\n",
    "\n",
    "    # Start ngrok tunnel\n",
    "    try:\n",
    "        print(f\"Starting ngrok tunnel for port {port}...\")\n",
    "        # Use bind_tls=True if you need HTTPS, but stick to False for simplicity if not required\n",
    "        public_url = ngrok.connect(port, \"http\", bind_tls=False).public_url\n",
    "        print(f\" FastAPI app is exposed at: {public_url}\")\n",
    "        print(f\"Access the FastAPI docs at: {public_url}/docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting ngrok tunnel: {e}\")\n",
    "        print(\"Please ensure ngrok is installed, authenticated, and not already running on this port.\")\n",
    "        public_url = None # Set to None if ngrok fails\n",
    "\n",
    "    # Run the FastAPI application using uvicorn in a separate thread\n",
    "    # Only start the thread if ngrok started successfully or if you intend to access locally\n",
    "    if public_url or not ngrok_auth_token: # Added condition to allow local run without ngrok token\n",
    "        try:\n",
    "            print(\"Starting Uvicorn server in a separate thread...\")\n",
    "            # Use reload=False when running with ngrok to avoid issues\n",
    "            # daemon=True allows the thread to exit when the main program exits\n",
    "            uvicorn_thread = Thread(target=uvicorn.run,\n",
    "                                    kwargs={\"app\": app, \"host\": \"0.0.0.0\", \"port\": port, \"reload\": False},\n",
    "                                    daemon=True)\n",
    "            uvicorn_thread.start()\n",
    "            print(\"Uvicorn server thread started. The script can now continue.\")\n",
    "            # If running as a script, you might want to add a line here to keep the main thread alive,\n",
    "            # e.g., input(\"Press Enter to stop the server...\\n\") or a loop\n",
    "            # In Colab, the notebook environment keeps the script alive.\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting uvicorn thread: {e}\")\n",
    "            print(\"Please check if the port is already in use.\")\n",
    "    else:\n",
    "        print(\"ngrok failed to start. Uvicorn server will not be started automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TYQxL8lHjGze"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"http://3cfe-34-125-125-99.ngrok-free.app/generate_story/\"\n",
    "\n",
    "# payload = {\n",
    "#     \"concept\": \"Ø§ÛŒØ³ÛŒ Ú©ÛØ§Ù†ÛŒ Ù„Ú©Ú¾ÛŒÚº Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© Ø´Ø®Øµ Ø§Ù†ØªÛØ§Ø¦ÛŒ ØºØ±ÛŒØ¨ ÛÙˆØªØ§ ÛÛ’Û” ÙˆÛ Ø§Ù¾Ù†ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ø§Ù…ÛŒØ± Ø¨Ù†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÙˆÙ‚Ù Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† Ø¬Ø¨ ÙˆÛ Ø¯ÙˆÙ„Øª Ù…Ù†Ø¯ Ø¨Ù† Ø¬Ø§ØªØ§ ÛÛ’ ØªÙˆ Ø§Ø³Û’ Ø§Ø­Ø³Ø§Ø³ ÛÙˆØªØ§ ÛÛ’ Ú©Û Ø§Ø³ Ù†Û’ Ø§Ù¾Ù†ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒÚº Ú©ÛŒØ§ Ú©Ú¾Ùˆ Ø¯ÛŒØ§ ÛÛ’Û”\",\n",
    "#     \"initial_story\": \"\",\n",
    "#     \"max_steps\": 9\n",
    "# }\n",
    "\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "#     response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "#     print(\"Response Status Code:\", response.status_code)\n",
    "#     print(\"Response Body:\", response.json())\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"An error occurred during the request: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KTFfJQfnMci"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
