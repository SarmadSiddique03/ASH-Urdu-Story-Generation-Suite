{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb425f39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cb28b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install fastapi nest_asyncio uvicorn pyngrok diffusers transformers torch accelerate python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3d070",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ngrok config add-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d4a21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# âœ… Auto-detect best dtype\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# âœ… Use 4-bit quantization if on GPU (reduces VRAM usage)\n",
    "load_in_4bit = torch.cuda.is_available()\n",
    "\n",
    "# âœ… Load the fine-tuned model and tokenizer\n",
    "model_name = \"sarmadsiddiqui29/Llama-3.1-8B-Instruct-Urdu-Story\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    ")\n",
    "\n",
    "# âœ… Ensure model is on the correct device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# âœ… Confirm everything is set correctly\n",
    "print(f\"Model loaded on {device} with dtype={dtype} (4-bit={load_in_4bit})\")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00392d2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from unsloth import FastLanguageModel\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import asyncio\n",
    "from threading import Thread # Import Thread\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in environments like notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "\n",
    "def fix_spacing(text):\n",
    "    \"\"\"Fix missing spaces in Urdu text.\"\"\"\n",
    "    # Using the updated regex for Urdu characters\n",
    "    return re.sub(r'(?<=[Ø€-Û¿])(?=[Ø€-Û¿])', ' ', text)\n",
    "\n",
    "def extract_text_after_last_story(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text after the last occurrence of \"Story:\" and ensures it ends with \"Û”\"\n",
    "    \"\"\"\n",
    "    matches = [m.end() for m in re.finditer(r'(?i)Story:', text)]\n",
    "    if matches:\n",
    "        extracted_text = text[matches[-1]:].strip()\n",
    "        # Using the correct Urdu full stop\n",
    "        last_full_stop = extracted_text.rfind(\"Û”\")\n",
    "        if last_full_stop != -1:\n",
    "            return extracted_text[:last_full_stop + 1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def remove_duplicate_sentences(text: str) -> str:\n",
    "    \"\"\"Removes duplicate sentences from the text based on the Urdu full stop 'Û”'.\"\"\"\n",
    "    # Using the correct Urdu full stop\n",
    "    sentences = text.split(\"Û”\")\n",
    "    seen = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence not in seen:\n",
    "            seen.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    # Using the correct Urdu full stop for joining\n",
    "    return \"Û” \".join(cleaned_sentences) + \"Û”\" if cleaned_sentences else \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Story Generation Function\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def generate_story_outline(concept: str, initial_story: str = \"\", max_steps: int = 9) -> str:\n",
    "    \"\"\"\n",
    "    Generates a structured, coherent, and grammatically sound Urdu story iteratively.\n",
    "    Uses a step-wise template to build narrative depth, with explicit instructions to ensure\n",
    "    a complete and engaging journey that concludes definitively.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None or device is None:\n",
    "        return \"Model or tokenizer not loaded. Cannot generate story. Please check your environment setup and model paths/permissions.\"\n",
    "\n",
    "    story_text = initial_story\n",
    "    complete_story = initial_story\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        if step == 1:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "[ðŸ”¹ Ø¢ØºØ§Ø² Ú©Ø±ÛŒÚº:\n",
    "- Ø¬Ù…Ù„Û’ Ú†Ú¾ÙˆÙ¹Û’ Ø§ÙˆØ± Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª ÛÙˆÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ø§Ø³Ø±Ø§Ø± Ø§ÙˆØ± Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”\n",
    "- ØµØ±Ù Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ú©ÛØ§Ù†ÛŒ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ø§ Ø¯Ù„Ú©Ø´ Ø¢ØºØ§Ø² Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 2:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø¢Ú¯Û’ Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚº:\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©Û’ Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ø§Ø±Ø§Ø¯ÙˆÚº Ø§ÙˆØ± Ú†Ú¾Ù¾Û’ Ø±Ø§Ø²ÙˆÚº Ú©Ùˆ Ù†Ù…Ø§ÛŒØ§Úº Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ù…Ø²ÛŒØ¯ Ø³ÙˆØ§Ù„Ø§Øª Ø§ÙˆØ± Ø§Ø³Ø±Ø§Ø± Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù†ÛŒØ§ Ù…ÙˆÚ‘ Ø¯ÛŒÚº Ø§ÙˆØ± Ú¯ÛØ±Ø§Ø¦ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 3:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ ØªÙ†Ø§Ø¤ Ø§ÙˆØ± Ú©Ø´Ù…Ú©Ø´ Ú©Ùˆ Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚº:\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ú©Ø´Ù…Ú©Ø´ Ø§ÙˆØ± ØºÛŒØ± Ù…ØªÙˆÙ‚Ø¹ Ù…ÙˆÚ‘ Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø§ÙˆØ± Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ø§ÛŒØ³Û’ Ù…Ù‚Ø§Ù… Ù¾Ø± Ù„Û’ Ø¬Ø§Ø¦ÛŒÚº Ø¬ÛØ§Úº Ù‚Ø§Ø±ÛŒ Ø­ÛŒØ±Ø§Ù† Ø±Û Ø¬Ø§Ø¦Û’Û”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 4:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ø¹Ø±ÙˆØ¬ Ú©ÛŒ Ø·Ø±Ù Ø¨Ú‘Ú¾ÛŒÚº:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ø³Ù†Ø³Ù†ÛŒ Ø®ÛŒØ²ÛŒ Ø§ÙˆØ± Ù†Ø¦Û’ Ø§Ù†Ú©Ø´Ø§ÙØ§Øª Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©Ø±Ø¯Ø§Ø±ÙˆÚº Ú©ÛŒ Ø¬Ø¯ÙˆØ¬ÛØ¯ Ø§ÙˆØ± Ù…Ù‚Ø§Ø¨Ù„Û’ Ú©Ùˆ ÙˆØ§Ø¶Ø­ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ø¹Ø±ÙˆØ¬ Ù¾Ø± Ù¾ÛÙ†Ú†Ø§Ø¦ÛŒÚº Ø¬ÛØ§Úº ÛØ± Ù„Ù…Ø­Û Ù†ÛŒØ§ Ø§Ù†Ú©Ø´Ø§Ù ÛÙˆÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 5:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ù…ÙˆÚ‘ Ø§ÙˆØ± Ù†ÛŒØ§ Ø±Ø®:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù†Ø¦Û’ Ø§ÙˆØ± ØºÛŒØ± Ù…ØªÙˆÙ‚Ø¹ Ù…ÙˆÚ‘ Ù¾Ø± Ù„Û’ Ø¬Ø§Ø¦ÛŒÚºÛ”\n",
    "- Ù…Ø²Ø§Ø­Ù…Øª Ø§ÙˆØ± Ú†ÛŒÙ„Ù†Ø¬Ø² Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ù…ÛŒÚº Ù†ÛŒØ§ Ø±Ø® Ø§ÙˆØ± Ù…Ø²ÛŒØ¯ Ú©Ø´Ù…Ú©Ø´ Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 6:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ø§Ø®ØªØªØ§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ú©ÛŒ Ø·Ø±Ù Ø¨Ú‘Ú¾ÛŒÚº:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ù…Ø²ÛŒØ¯ ØªÙØµÛŒÙ„ Ø³Û’ Ø¨ÛŒØ§Ù† Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ú†ÛŒÙ„Ù†Ø¬Ø² Ú©ÛŒ Ø´Ø¯Øª Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚºÛ”\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒ Ø¬Ø¯ÙˆØ¬ÛØ¯ Ú©Ùˆ Ú¯ÛØ±Ø§Ø¦ÛŒ Ø³Û’ Ù¾ÛŒØ´ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§Ø®ØªØªØ§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ú©ÛŒ Ø·Ø±Ù Ù„Û’ Ø¬Ø§Ø¦ÛŒÚºØŒ Ù…Ú¯Ø± Ø§ÛŒÚ© Ø¢Ø®Ø±ÛŒ Ø­ÛŒØ±Ø§Ù† Ú©Ù† Ù…ÙˆÚ‘ Ú†Ú¾ÙˆÚ‘ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 7:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ù…Ú©Ù…Ù„ Ø§Ø®ØªØªØ§Ù… Ú©ÛŒ ØªÛŒØ§Ø±ÛŒ:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù…Ø±Ø¨ÙˆØ· Ø§ÙˆØ± Ù…Ú©Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ú©ÛŒ Ø·Ø±Ù Ù„Û’ Ø¬Ø§Ø¦ÛŒÚºÛ”\n",
    "- ØªÙ…Ø§Ù… Ø§ÛÙ… Ù…ÙˆÚ‘ Ø§ÙˆØ± Ú©Ø´Ù…Ú©Ø´ Ú©Ùˆ Ø­Ù„ Ú©Ø±ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø´Ø§Ù†Ø¯Ø§Ø± Ø§Ù†Ø¬Ø§Ù… ØªÚ© Ù¾ÛÙ†Ú†Ø§Ø¦ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 8:\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù…:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¢Ø®Ø±ÛŒ ØªØ§Ø«Ø±Ø§Øª Ú†Ú¾ÙˆÚ‘ÛŒÚºÛ”\n",
    "- ØºÛŒØ± Ø¶Ø±ÙˆØ±ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ú©Ùˆ Ø­Ø°Ù Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…Ø±Ú©Ø²ÛŒ Ú©ÛØ§Ù†ÛŒ Ù¾Ø± ØªÙˆØ¬Û Ø¯ÛŒÚºÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© ÙˆØ§Ø¶Ø­ Ø§ÙˆØ± Ù…Ú©Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ú©Û’ Ø³Ø§ØªÚ¾ Ø®ØªÙ… Ú©Ø±ÛŒÚºÛ”**\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "        elif step == 9:  # Final polishing step with lower temperature for coherence\n",
    "            template = f\"\"\"\n",
    "**ðŸ’¡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„:** {concept}\n",
    "\n",
    "Ú©ÛØ§Ù†ÛŒ:\n",
    "{story_text}\n",
    "\n",
    "[ðŸ”¹ Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ØŒ Ù…Ø±Ø¨ÙˆØ· Ø§ÙˆØ± Ø¯Ù„Ú©Ø´ Ø§Ù†Ø¬Ø§Ù… ØªÚ© Ù¾ÛÙ†Ú†Ø§Ø¦ÛŒÚº:\n",
    "- Ú©ÛØ§Ù†ÛŒ Ú©Û’ ØªÙ…Ø§Ù… Ø§ÛÙ… Ù…ÙˆÚ‘ Ø§ÙˆØ± Ú©Ø´Ù…Ú©Ø´ Ú©Ùˆ Ø­Ù„ Ú©Ø±ÛŒÚºÛ”\n",
    "- Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒ Ø°ÛÙ†ÛŒ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ±Ù‚ÛŒ Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ÛŒÚºÛ”\n",
    "- Ø±ÙˆØ²Ù…Ø±Û Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ø³Û’ ÛÙ¹ Ú©Ø± Ø§ÛŒÚ© Ø¯Ù„Ú†Ø³Ù¾ Ø³ÙØ± Ø§ÙˆØ± ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Ùˆ Ù…Ø±Ú©Ø²ÛŒ Ø­ÛŒØ«ÛŒØª Ø¯ÛŒÚºÛ”\n",
    "- Ú©ÛØ§Ù†ÛŒ Ø§ÛŒÚ© ÙˆØ§Ø¶Ø­ØŒ Ø§Ø«Ø± Ø§Ù†Ú¯ÛŒØ² Ø§Ø®ØªØªØ§Ù… Ù¾Ø± Ø®ØªÙ… ÛÙˆÛ”]\n",
    "\n",
    "ðŸ“œ **Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÛØ§Ù†ÛŒ Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø§ÙˆØ± Ù…Ø±Ø¨ÙˆØ· Ø§Ø®ØªØªØ§Ù… Ú©Û’ Ø³Ø§ØªÚ¾ Ø®ØªÙ… Ú©Ø±ÛŒÚºÛ”**\n",
    "ØµØ±Ù Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ú©ÛØ§Ù†ÛŒ Ù„Ú©Ú¾ÛŒÚº Ø§ÙˆØ± Ø§Ø¶Ø§ÙÛŒ ÛØ¯Ø§ÛŒØ§Øª Ø´Ø§Ù…Ù„ Ù†Û Ú©Ø±ÛŒÚºÛ”\n",
    "After this dont write indicators for users just Start the story in Urdu only after the word \"Story:\"\n",
    "            \"\"\"\n",
    "\n",
    "        # Prepare input tokens from the template\n",
    "        inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Adjust max tokens and temperature per step\n",
    "        max_tokens = 250\n",
    "        temperature = 0.7\n",
    "        if step >= 7:\n",
    "            max_tokens = 400\n",
    "        if step == 9:\n",
    "            max_tokens = 500\n",
    "            temperature = 0.6\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        # Extract the new text generated after the last occurrence of \"Story:\"\n",
    "        new_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        extracted_text = extract_text_after_last_story(new_text)\n",
    "\n",
    "        # Update story text for the next step\n",
    "        story_text = extracted_text\n",
    "        complete_story += \" \" + extracted_text\n",
    "\n",
    "    complete_story = remove_duplicate_sentences(complete_story)\n",
    "    return complete_story\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class StoryRequest(BaseModel):\n",
    "    concept: str\n",
    "    initial_story: str = \"\"\n",
    "    max_steps: int = 9\n",
    "\n",
    "@app.post(\"/generate_story/\")\n",
    "async def generate_story(request: StoryRequest):\n",
    "    try:\n",
    "        story = generate_story_outline(request.concept, request.initial_story, request.max_steps)\n",
    "        return {\"story\": story}\n",
    "    except Exception as e:\n",
    "        # Log the exception for debugging\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred during story generation: {e}\")\n",
    "\n",
    "# Code to run the FastAPI app with uvicorn and expose with ngrok\n",
    "# This pattern is suitable for environments like Google Colab or Jupyter notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the ngrok authtoken from environment variable\n",
    "    # Make sure to set the NGROK_AUTH_TOKEN environment variable\n",
    "    ngrok_auth_token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
    "    if ngrok_auth_token:\n",
    "        ngrok.set_auth_token(ngrok_auth_token)\n",
    "    else:\n",
    "        print(\"Warning: NGROK_AUTH_TOKEN environment variable not set. ngrok might not work.\")\n",
    "\n",
    "\n",
    "    # Define the port for FastAPI\n",
    "    port = 8000\n",
    "\n",
    "    # Start ngrok tunnel\n",
    "    try:\n",
    "        print(f\"Starting ngrok tunnel for port {port}...\")\n",
    "        # Use bind_tls=True if you need HTTPS, but stick to False for simplicity if not required\n",
    "        public_url = ngrok.connect(port, \"http\", bind_tls=False).public_url\n",
    "        print(f\" FastAPI app is exposed at: {public_url}\")\n",
    "        print(f\"Access the FastAPI docs at: {public_url}/docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting ngrok tunnel: {e}\")\n",
    "        print(\"Please ensure ngrok is installed, authenticated, and not already running on this port.\")\n",
    "        public_url = None # Set to None if ngrok fails\n",
    "\n",
    "    # Run the FastAPI application using uvicorn in a separate thread\n",
    "    # Only start the thread if ngrok started successfully or if you intend to access locally\n",
    "    if public_url or not ngrok_auth_token: # Added condition to allow local run without ngrok token\n",
    "        try:\n",
    "            print(\"Starting Uvicorn server in a separate thread...\")\n",
    "            # Use reload=False when running with ngrok to avoid issues\n",
    "            # daemon=True allows the thread to exit when the main program exits\n",
    "            uvicorn_thread = Thread(target=uvicorn.run,\n",
    "                                    kwargs={\"app\": app, \"host\": \"0.0.0.0\", \"port\": port, \"reload\": False},\n",
    "                                    daemon=True)\n",
    "            uvicorn_thread.start()\n",
    "            print(\"Uvicorn server thread started. The script can now continue.\")\n",
    "            # If running as a script, you might want to add a line here to keep the main thread alive,\n",
    "            # e.g., input(\"Press Enter to stop the server...\\n\") or a loop\n",
    "            # In Colab, the notebook environment keeps the script alive.\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting uvicorn thread: {e}\")\n",
    "            print(\"Please check if the port is already in use.\")\n",
    "    else:\n",
    "        print(\"ngrok failed to start. Uvicorn server will not be started automatically.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
